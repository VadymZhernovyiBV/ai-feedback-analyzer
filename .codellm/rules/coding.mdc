# AI Feedback Analyzer - Project Rules

This document contains project-wide rules that AI assistants must follow in every conversation about the AI Feedback Analyzer project. These rules ensure consistency, quality, and maintainability across all development work.

## 📌 Project Awareness

### Required Reading
- **ALWAYS** read `~/ai_feedback_analyzer_specification.md` before making any code changes
- Check existing project structure and understand the current implementation state
- Review any existing task lists, TODO comments, or project documentation
- Understand the tech stack: React 18 + TypeScript (frontend), Python 3.11+ FastAPI (backend), OpenRouter API integration

### Project Context Understanding
- This is a web-based customer feedback analysis platform using AI/ML
- Anonymous usage (no authentication required)
- Free hosting: Vercel (frontend) + Railway/Render (backend)
- Database: SQLite (dev) / PostgreSQL (prod)
- Key features: sentiment analysis, category classification, feedback history, tag management

### Before Starting Work
1. Confirm current project structure matches specification
2. Check if backend and frontend directories exist and are properly organized
3. Verify environment setup requirements are met
4. Review any existing code for patterns and conventions

## 🗂️ Code Structure

### File Size Limits
- **Python files**: Maximum 500 lines per file
- **TypeScript/React files**: Maximum 300 lines per component file
- **Configuration files**: Maximum 100 lines
- If files exceed limits, split into logical modules or components

### Module Organization

#### Backend Structure (Python/FastAPI)
```
backend/app/
├── main.py                 # Entry point (max 100 lines)
├── config.py              # Configuration (max 150 lines)
├── database.py            # DB setup (max 200 lines)
├── models/                # SQLAlchemy models
├── routers/               # API endpoints (max 200 lines per router)
├── services/              # Business logic (max 300 lines per service)
├── utils/                 # Helper functions (max 200 lines per util)
└── tests/                 # Test files
```

#### Frontend Structure (React/TypeScript)
```
frontend/src/
├── components/            # Reusable components (max 200 lines each)
├── pages/                 # Page components (max 250 lines each)
├── store/                 # Redux slices (max 300 lines per slice)
├── hooks/                 # Custom hooks (max 150 lines each)
├── utils/                 # Helper functions (max 200 lines each)
├── types/                 # TypeScript interfaces (max 300 lines each)
└── tests/                 # Test files
```

### Folder Structure Rules
- Group related functionality together
- Use consistent naming: kebab-case for files, PascalCase for React components
- Keep test files adjacent to source files or in dedicated test directories
- Separate concerns: UI components, business logic, data access, utilities

### Import Organization
- Group imports: external libraries, internal modules, relative imports
- Use absolute imports for src/ directory in frontend
- Use consistent import aliases (@/ for src in frontend)

## 🧪 Testing Requirements

### Coverage Expectations
- **Minimum 80% code coverage** for both backend and frontend
- **100% coverage required** for critical business logic (AI service integration, data validation)
- All API endpoints must have integration tests
- All React components must have unit tests

### Testing Frameworks

#### Backend (Python)
- **Primary**: pytest with pytest-asyncio
- **Database testing**: Use SQLite in-memory for tests
- **API testing**: FastAPI TestClient
- **Mocking**: pytest-mock for external services (OpenRouter API)

#### Frontend (TypeScript/React)
- **Primary**: Vitest (configured with Vite)
- **Component testing**: @testing-library/react
- **User interaction**: @testing-library/user-event
- **Mocking**: vi.mock() for API calls and external dependencies

### Test Patterns

#### Backend Test Structure
```python
# test_feedback_service.py
import pytest
from unittest.mock import AsyncMock
from app.services.feedback_service import FeedbackService

class TestFeedbackService:
    @pytest.fixture
    def mock_ai_service(self):
        return AsyncMock()
    
    @pytest.mark.asyncio
    async def test_analyze_feedback_success(self, mock_ai_service):
        # Test implementation
        pass
    
    @pytest.mark.asyncio
    async def test_analyze_feedback_ai_failure_fallback(self, mock_ai_service):
        # Test fallback behavior
        pass
```

#### Frontend Test Structure
```typescript
// FeedbackForm.test.tsx
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import { Provider } from 'react-redux';
import { FeedbackForm } from './FeedbackForm';
import { store } from '../store';

describe('FeedbackForm', () => {
  const renderWithProvider = (component: React.ReactElement) => {
    return render(
      <Provider store={store}>
        {component}
      </Provider>
    );
  };

  it('should submit feedback successfully', async () => {
    // Test implementation
  });

  it('should show validation errors for invalid input', async () => {
    // Test implementation
  });
});
```

### Test Requirements
- Test happy path and error scenarios
- Mock external dependencies (OpenRouter API, database)
- Test edge cases and boundary conditions
- Include integration tests for API endpoints
- Test React component rendering and user interactions

## 📎 Style & Conventions

### Python/FastAPI Backend

#### Code Formatting
- **Black**: Line length 88 characters
- **isort**: Import sorting with profile "black"
- **Ruff**: Linting with aggressive settings
- **mypy**: Type checking enabled

#### Naming Conventions
```python
# Variables and functions: snake_case
feedback_text = "example"
def analyze_sentiment(text: str) -> dict:
    pass

# Classes: PascalCase
class FeedbackService:
    pass

# Constants: UPPER_SNAKE_CASE
MAX_TEXT_LENGTH = 5000
DEFAULT_MODEL = "openai/gpt-3.5-turbo"

# Private methods: _leading_underscore
def _validate_input(self, text: str) -> bool:
    pass
```

#### Type Hints
- **Required** for all function parameters and return types
- Use `typing` module for complex types
- Use `Optional` for nullable values
- Use `Union` sparingly, prefer specific types

```python
from typing import Optional, List, Dict, Any
from pydantic import BaseModel

async def analyze_feedback(
    text: str,
    tags: Optional[List[str]] = None
) -> Dict[str, Any]:
    pass
```

#### Error Handling
```python
# Use specific exception types
from app.utils.exceptions import ValidationError, AIServiceError

# Proper exception handling
try:
    result = await ai_service.analyze(text)
except httpx.TimeoutException:
    raise AIServiceError("AI service timeout")
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    raise
```

### React/TypeScript Frontend

#### Code Formatting
- **Prettier**: 2 spaces, single quotes, trailing commas
- **ESLint**: TypeScript recommended + React hooks rules
- **Line length**: 100 characters

#### Naming Conventions
```typescript
// Components: PascalCase
const FeedbackForm: React.FC<FeedbackFormProps> = () => {};

// Variables and functions: camelCase
const feedbackText = "example";
const handleSubmit = (data: FormData) => {};

// Constants: UPPER_SNAKE_CASE
const MAX_TEXT_LENGTH = 5000;
const API_BASE_URL = process.env.VITE_API_BASE_URL;

// Types and interfaces: PascalCase
interface FeedbackEntry {
  id: number;
  text: string;
  sentiment: 'positive' | 'neutral' | 'negative';
}

// Enums: PascalCase
enum SentimentType {
  Positive = 'positive',
  Neutral = 'neutral',
  Negative = 'negative'
}
```

#### Component Structure
```typescript
// Component file structure
import React from 'react';
import { useSelector, useDispatch } from 'react-redux';
import { Button, TextField } from '@mui/material';
import { FeedbackFormProps } from '../types/feedback';

// Component implementation
export const FeedbackForm: React.FC<FeedbackFormProps> = ({
  onSubmit,
  loading = false
}) => {
  // Hooks first
  const dispatch = useDispatch();
  const feedbackState = useSelector(state => state.feedback);
  
  // Event handlers
  const handleSubmit = useCallback((data: FormData) => {
    onSubmit(data);
  }, [onSubmit]);
  
  // Render
  return (
    <form onSubmit={handleSubmit}>
      {/* JSX content */}
    </form>
  );
};
```

#### TypeScript Rules
- **Strict mode enabled** in tsconfig.json
- No `any` types unless absolutely necessary
- Use proper interface definitions for all props
- Prefer `interface` over `type` for object shapes
- Use generic types for reusable components

```typescript
// Good: Specific types
interface FeedbackCardProps {
  feedback: FeedbackEntry;
  onTagUpdate?: (id: number, tags: string[]) => void;
  compact?: boolean;
}

// Avoid: any types
const handleData = (data: any) => {}; // ❌

// Good: Proper typing
const handleData = (data: FeedbackEntry) => {}; // ✅
```

### CSS/Styling
- Use Material-UI theme system consistently
- Prefer `sx` prop over custom CSS classes
- Use theme breakpoints for responsive design
- Follow Material Design principles

## 📚 Documentation & Explainability

### Docstring Format (Python)
Use **Google style** docstrings:

```python
async def analyze_feedback(
    text: str,
    tags: Optional[List[str]] = None
) -> Dict[str, Any]:
    """Analyze customer feedback using AI service.
    
    Args:
        text: The feedback text to analyze (max 5000 characters).
        tags: Optional list of tags to associate with feedback.
        
    Returns:
        Dictionary containing sentiment, category, confidence scores,
        and analysis metadata.
        
    Raises:
        ValidationError: If text is empty or exceeds length limit.
        AIServiceError: If AI service is unavailable or returns invalid response.
        
    Example:
        >>> result = await analyze_feedback("Great product!")
        >>> print(result['sentiment'])
        'positive'
    """
```

### JSDoc Format (TypeScript)
```typescript
/**
 * Feedback form component for submitting customer feedback.
 * 
 * @param props - Component props
 * @param props.onSubmit - Callback function called when form is submitted
 * @param props.loading - Whether the form is in loading state
 * 
 * @returns React functional component
 * 
 * @example
 * ```tsx
 * <FeedbackForm 
 *   onSubmit={handleSubmit} 
 *   loading={isAnalyzing} 
 * />
 * ```
 */
export const FeedbackForm: React.FC<FeedbackFormProps> = ({
  onSubmit,
  loading = false
}) => {
  // Component implementation
};
```

### Code Comments
- Explain **why**, not **what**
- Document complex business logic
- Add TODO comments with context and assignee
- Use inline comments sparingly, prefer self-documenting code

```python
# Good: Explains reasoning
# Use fallback model if primary model fails to ensure service availability
if primary_model_failed:
    result = await self._analyze_with_fallback(text)

# Avoid: States the obvious
# Set the text variable to the input text
text = input_text
```

### README Maintenance
- Keep installation instructions current
- Document environment variables and configuration
- Include API documentation links
- Provide troubleshooting section
- Update deployment instructions when changed

### API Documentation
- Use FastAPI automatic documentation
- Add detailed descriptions to Pydantic models
- Include example requests/responses
- Document error codes and meanings

## 🤖 AI Behavior Rules

### OpenRouter API Integration
- **Always implement fallback mechanisms** for AI service failures
- Use rule-based analysis as ultimate fallback
- Implement proper rate limiting (100 req/min for analysis)
- Cache results to avoid duplicate API calls
- Monitor API usage and costs

### Error Handling for AI Services
```python
# Required pattern for AI service calls
async def analyze_with_ai(text: str) -> Dict[str, Any]:
    try:
        # Primary AI service call
        result = await openrouter_client.analyze(text)
        return result
    except httpx.TimeoutException:
        logger.warning("AI service timeout, using fallback")
        return await fallback_analysis(text)
    except Exception as e:
        logger.error(f"AI service error: {e}")
        return await rule_based_fallback(text)
```

### Prompt Engineering Rules
- Keep prompts concise and specific
- Always request JSON format responses
- Include confidence scores in AI responses
- Validate AI responses before using them
- Log AI interactions for debugging

### Model Selection Strategy
1. **Primary**: `openai/gpt-3.5-turbo` (paid, higher quality)
2. **Fallback**: `meta-llama/llama-3.1-8b-instruct:free` (free tier)
3. **Ultimate fallback**: Rule-based analysis (no API required)

### AI Response Validation
```python
def validate_ai_response(response: Dict[str, Any]) -> bool:
    """Validate AI service response format and content."""
    required_fields = ['sentiment', 'confidence', 'category']
    
    # Check required fields exist
    if not all(field in response for field in required_fields):
        return False
    
    # Validate sentiment values
    valid_sentiments = ['positive', 'neutral', 'negative']
    if response['sentiment'] not in valid_sentiments:
        return False
    
    # Validate confidence range
    confidence = response.get('confidence', 0)
    if not (0.0 <= confidence <= 1.0):
        return False
    
    return True
```

### Performance Considerations
- Implement request queuing for rate limits
- Use async/await for all AI service calls
- Set appropriate timeouts (30 seconds max)
- Implement circuit breaker pattern for repeated failures
- Cache frequently analyzed text patterns

### Security Rules
- Never log sensitive user data
- Sanitize input before sending to AI services
- Validate all AI responses before storing
- Implement input length limits (5000 characters max)
- Use environment variables for API keys

### Development Workflow
1. **Always test AI integration** with mock responses first
2. **Implement offline mode** for development without API keys
3. **Use free tier models** for development and testing
4. **Monitor API costs** in production
5. **Implement graceful degradation** when AI services are unavailable

---

## 🔧 Development Commands

### Backend
```bash
# Setup
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Development
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Testing
pytest --cov=app --cov-report=html
python -m pytest tests/ -v

# Linting
black app/ tests/
isort app/ tests/
ruff check app/ tests/
mypy app/
```

### Frontend
```bash
# Setup
npm install

# Development
npm run dev

# Testing
npm run test
npm run test:coverage

# Linting
npm run lint
npm run format
npm run type-check

# Build
npm run build
```

---

**Remember**: These rules ensure consistency, maintainability, and quality across the AI Feedback Analyzer project. Always refer back to this document when working on any aspect of the codebase.